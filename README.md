# <u>**t2yLLM : a fast LLM based Voice Assistant**</u>

![Python](https://img.shields.io/badge/Python-3.12-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?logo=pytorch&logoColor=white)
![vLLM](https://img.shields.io/badge/vLLM-Inference-green)
![FasterWhisper](https://img.shields.io/badge/FasterWhisper-STT-green)
![Raspberry Pi](https://img.shields.io/badge/RaspberryPi-5-green)

## <u>üí° What it does </u>

- **t2yLLM** lets you speak to your device of choice (here a raspberry Pi with a respeaker hat from seeed studio)
and get an audio answer from your favorite LLM (here Qwen3 by default).
It should just work like any home assistant.
The default keyword to activate speech detection is **"Ok Mars"**
but you can change it of course.
ATM if you want a custom keyword, it is **mandatory** to create a Picovoice account (check [Picovoice](https://picovoice.ai/)), train and download a custom keyword to get a working pipeline.
- **Meteo** : It can search for meteo infos using your OpenWeather API key
- **Pokemon** : Look for any Pokemon info using Tyradex API (french) or PokeApi (english, others...)
- **Wikipedia** : Make Wikipedia searches using the python API
- **Vector Search** : stores all in a synthetic way in chromadb if needed and can retrieve the memorized info
- **t2yLLM** is meant to work on a 16GB GPU, but in order to achieve that, first launch the LLM backend script in order to avoid OOM

## <u>üöÄ Quickstart</u>

- Install pytorch for your cuda version (see https://pytorch.org/get-started/locally/) :<br>
  `pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128`<br>
  for cuda 12.8. <br>
  For now on Blackwell GPUs, you need to uninstall current and install nightly :<br>
  `pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128
  
- Install vllm : `pip install vllm --extra-index-url https://download.pytorch.org/whl/cu128`<br>
  if you have Blackwell, you should follow : `https://docs.vllm.ai/en/stable/getting_started/installation/gpu.html#use-an-existing-pytorch-installation`<br>
  ```bash
  git clone https://github.com/vllm-project/vllm.git
  cd vllm
  python use_existing_torch.py
  pip install -r requirements/build.txt
  pip install --no-build-isolation -e .```
  

- Install flash attention [flash-attention](https://github.com/Dao-AILab/flash-attention)

- Clone the repository :
  `git clone https://github.com/Saga9103/t2yLLM.git`<br>
  `cd t2yLLM`<br>
  `pip install -e .`

- Open the necessary ports as marked in config files for UDP

- See examples in **./examples** on how to use and import

- **AssistantEngine** class receives user prompts (text/str) generated via Faster-Whisper, browses APIs if needed, generates an answer
  (token by token with the async engine of vllm) and forwards it to the dispatcher.
  the related python script should be installed on your server/desktop.
 ![llm](https://github.com/user-attachments/assets/d15d89c2-ce67-4571-b4b5-cb123fb0be7b)


  
- the **VoiceEngine** class runs Faster-Whisper, piperTTS, silero and porcupine. It is responsible for :
  - transforming answers generated by llm_backend_async.py to audio chunks (in .flac via piperTTS) and send them over udp to your
  raspberry Pi.
  - getting audio from the raspberry Pi and converting it to text via Faster-Whisper with as low latency as possible.
  this script should be installed on your server/desktop (by default, the same as llm_backend_async.py but it can be different).
![dispatcher](https://github.com/user-attachments/assets/63539894-c2ec-486a-a2fd-2eab4d055220)

  
- the **rpi_server.py** program should be copied and run on the raspberry Pi. It is responsible for :
  - getting audio from the respeaker lite and forwarding it to **VoiceEngine**
  - getting audio (.flac chunks) generated by dispatcher.py and play it the speaker linked to your respeaker lite (or device of choice)
  
- the .yaml config files should be used to tweak paramaters like silence, models, directories etc...

- You can check models I use in the config files and also in the faster_whisper directory. **To make things work on a 16GB gpu 
it needs quantization. You also need to fully load the llm_back_async.py first** via llm_example.py. Should have no problem on 24GB GPUs.

- Different parameters of vLLM can be used to save VRAM like enforce_eager, max_model_len etc... vLLM documentation is very rich

![t2yLLM](https://github.com/user-attachments/assets/21c1988d-dd92-48d8-8632-fe34aa4b4188)



## <u>üî• Backends</u>

- **vLLM** : really fast and well documented inference pipeline for your favorite LLM
- **Faster-Whisper** : incredibly fast STT for realtime text generation
- **piperTTS** : fast text to speech generating natural voice, maybe the best for french atm
- **Silero-vad** : process the audio buffer and prevents whisper hallucinations
- **pvporcupine** : keyword detection
- **Chromadb** : a vector search database that serves as the model memory
- **default LLM** : Qwen3 14B or other variants, GPTQ 4Bit quantized
- **Pytorch**

## <u>üí° Specifics</u>

###  Material :
  - **client side** :
    - Raspberry Pi 5, 4GB
    - respeaker like from seeed studio
  - **server side** :
    - An Nvidia GPU with 16GB of VRAM (mini)

###  Pipeline :
  - **t2yLLM** uses AsyncLLMEngine in order to stream tokens and generate sound from them as soon as possible.
  - The audio dispatcher processes text received from the LLM and transforms it to .flac segments and then
    sends them to the client (raspberry Pi)
  - Sound reveived from the Pi 5 is analyzed by silerovad to detect speech in addition to pvporcupine
  - Relevant sound is then translated by Faster-Whisper with low latency
  - The audio dispatcher transforms the LLM answer to speech with coqui TTS and then sends audio parts in .flac
    over the network to reduce bandwidth usage and decrease latency

## <u>‚öôÔ∏è Parameters</u>

- configuration should be done via the .yaml config file without having to directly interact with the code
- configuration can be enhanced via the YamlConfigLoader.py
- **t2yLLM** should be used on local network only since all is in clear text for now

## <u>‚öôÔ∏è Environment variables</u>

create a .env file and use python-dotenv or edit your ~/.bashrc :

- export PORCUPINE_KEY='myporcupinekey'
- export OPENWEATHERMAP_API_KEY='myopenweatherkey'
- export VLLM_ATTENTION_BACKEND=FLASH_ATTN #for V1 engine
- export VLLM_FLASH_ATTN_VERSION="2"       #for V1 engine
- export VLLM_USE_V1=1
- VLLM_WORKER_MULTIPROC_METHOD="spawn" #for V1 engine
- export TORCH_CUDA_ARCH_LIST='myarchitecture' #if needed


## <u>üîç Github repositories used in order to make this code</u>

- üîó [vLLM](https://github.com/vllm-project/vllm)
- üîó [Faster-Whisper](https://github.com/SYSTRAN/faster-whisper)
- üîó [Silero-vad](https://github.com/snakers4/silero-vad)
- üîó [pvporcupine](https://github.com/Picovoice/porcupine)
- üîó [Whisper-streaming](https://github.com/ufal/whisper_streaming)
- üîó [RealtimeSTT](https://github.com/KoljaB/RealtimeSTT)
- üîó [Chromadb](https://github.com/chroma-core/chroma)
- üîó [piperTTS](https://github.com/rhasspy/piper)
- üîó [json_repair](https://github.com/mangiucugna/json_repair)

## <u>üîç APIs</u>

- üîó [Tyradex](https://tyradex.vercel.app/)
- üîó [pokeapi](https://github.com/PokeAPI/pokeapi)
- üîó [OpenWeather](https://openweathermap.org/)

## <u>üõ†Ô∏è ToDo</u>

- Keep context between interactions
- improve processing pipeline
- English pokemon phonetics
- Add the ability to change user agent names in .yaml config files
- Add the ability to use either UDP or Quic 
- switch or add option to use OpenWakeWord with available custom model weights (but I could not compile properly and make it train on a custom wake word)

## <u>‚öñÔ∏è License</u>

This code is under the **MIT** license. Please mention me as the author if you found this code useful
  [![MIT License](https://img.shields.io/badge/license-MIT-green)](https://opensource.org/licenses/MIT)

Copyright (c) 2025 Saga9103

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
