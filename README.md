# **t2yLLM : a fast LLM based Voice Assistant**

![Python](https://img.shields.io/badge/Python-3.12-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?logo=pytorch&logoColor=white)
![vLLM](https://img.shields.io/badge/vLLM-Inference-green)
![FasterWhisper](https://img.shields.io/badge/FasterWhisper-STT-green)
![Raspberry Pi](https://img.shields.io/badge/RaspberryPi-5-green)

## üí° What it does

  üó£Ô∏è speak to your device of choice
  and get an audio answer from your favorite LLM (here Qwen3 by default)<br>
  Or just interact via keyboard ‚å®Ô∏è <br>
  It should just work like any home assistant.<br>
  The default keyword to activate speech detection is **"Ok Mars"**

- **Two modes** :
  
  - **Local** : one PC equiped with a microphone and a speaker. Compatible with :
      - **Jabra Speak2**
      - **Seeed Studio respeaker mic array V2.0**
        
  - **Distributed** :
    can be divided in :
      - 1 PC for LLM & Whisper + raspberry PI listener (or any device on wlan)
      - 1 PC for LLM + 1 PC for Whisper + raspberry PI listener (or any device on wlan)
        
- ATM if you want a custom keyword, it is **mandatory** to create a Picovoice account (check [Picovoice](https://picovoice.ai/)), train and download a custom keyword to get a working pipeline.
  - **Meteo** : It can search for meteo infos using your OpenWeather API key
  - **Pokemon** : Look for any Pokemon info using Tyradex API (french) or PokeApi (english, others...)
  - **Wikipedia** : Make Wikipedia searches using the python API
  - **Vector Search** : stores all in a synthetic way in chromadb if needed and can retrieve the memorized info
  - **t2yLLM** is meant to work on a 16GB GPU, but in order to achieve that, first launch the LLM backend script in order to avoid OOM

## üöÄ Quickstart

- [Backends](#-backends) <br>
- [Specifics](#-specifics) <br>
- [Pipeline](#pipeline-) <br>
- [Parameters](#%EF%B8%8F-parameters) <br>
- [Environment variables](#%EF%B8%8F-environment-variables) <br>
- [Links](#-github-links) <br>
- [APIs](#-apis) <br>
- [Plugins](#plugins) <br>
  - [Plugins Setup](https://github.com/Saga9103/t2yLLM/blob/main/t2yLLM/plugins/README.md) <br>
- [ToDo](#%EF%B8%8F-todo) <br>


### I/ Install
- Install pytorch for your cuda version (see https://pytorch.org/get-started/locally/) :<br>
  `pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128 `<br>
  for cuda 12.8. <br>
  For now on Blackwell GPUs, you need to uninstall current and install nightly :<br>
  `pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128`
  
- Install vllm : `pip install vllm --extra-index-url https://download.pytorch.org/whl/cu128`<br>
  if you have Blackwell, you should follow : `https://docs.vllm.ai/en/stable/getting_started/installation/gpu.html#use-an-existing-pytorch-installation`<br>
  ```bash
  git clone https://github.com/vllm-project/vllm.git
  cd vllm
  python use_existing_torch.py
  pip install -r requirements/build.txt
  pip install --no-build-isolation -e .
  

- (Optional) Install flash attention 2 [flash-attention](https://github.com/Dao-AILab/flash-attention)

- Clone the repository :
  `git clone https://github.com/Saga9103/t2yLLM.git`<br>
  `cd t2yLLM`<br>
  `pip install -e .`

### II/ Configure
-  go to the main t2yLLM directory and run the hmac generator **hmac_generator.py**
-  save the key in your envirronement variables (edit ~/.bashrc or your .env file) 

- If on **local mode** (one PC only, speaker+mic linked via USB):
  - nothing to do
- If multiple devices :
  - Open the necessary ports as marked in config files for UDP

### III/ Basic examples
- See examples in **./examples** on how to use and import :
  - **home_assistant.py** is the **local** mode, tested with Jabra Speak2
  - others are for **distributed** arch, where a **Raspberry-Pi + respeaker** is used

- **AssistantEngine** class receives user prompts (text/str) generated via Faster-Whisper, browses APIs if needed, generates an answer
  (token by token with the async engine of vllm) and forwards it to the dispatcher.
  the related python script should be installed on your server/desktop.
 ![llm](https://github.com/user-attachments/assets/d15d89c2-ce67-4571-b4b5-cb123fb0be7b)


  
- the **VoiceEngine** class runs **Faster-Whisper, piperTTS, silero-vad and porcupine**. It is responsible for :
  - transforming answers generated by llm_backend_async.py to audio chunks (in .flac via piperTTS) and send them over udp to your
  raspberry Pi.
  - getting audio from the raspberry Pi and converting it to text via Faster-Whisper with as low latency as possible.
  this script should be installed on your server/desktop (by default, the same as llm_backend_async.py but it can be different).
![dispatcher](https://github.com/user-attachments/assets/63539894-c2ec-486a-a2fd-2eab4d055220)

- if you are using distributed mode :
  - the **rpi_server.py** program should be copied and run on the raspberry Pi. It is responsible for :
    - getting audio from the respeaker lite and forwarding it to **VoiceEngine**
    - getting audio (.flac chunks) generated by dispatcher.py and play it the speaker linked to your respeaker lite (or device of choice)

### IV/ Configuration
- the .yaml config files should be used to tweak paramaters like silence, models, directories etc...

- You can check models I use in the config files and also in the faster_whisper directory. **To make things work on a 16GB gpu 
it needs quantization. You also need to fully load the llm_back_async.py first** via llm_example.py. Should have no problem on 24GB GPUs.

- Different parameters of vLLM can be used to save VRAM like enforce_eager, max_model_len etc... vLLM documentation is very rich

  - Local mode :
![LocalDispatcher](https://github.com/user-attachments/assets/9bae0833-534f-4aeb-a28a-1bedb0eff61b)
  - Distributed mode :
![t2yLLM](https://github.com/user-attachments/assets/21c1988d-dd92-48d8-8632-fe34aa4b4188)

- **Web UI** for interactive talk and displaying code or math formulas /examples/llm_example_webui.py :
![t2yLLM_webui](https://github.com/user-attachments/assets/ed82494c-cd34-4cd1-9cbf-74ff0f9d3059)


## üî• Backends

- **vLLM** : really fast and well documented inference pipeline for your favorite LLM
- **Faster-Whisper** : incredibly fast STT for realtime text generation
- **piperTTS** : fast text to speech generating natural voice, maybe the best for french atm
- **Silero-vad** : process the audio buffer and prevents whisper hallucinations
- **pvporcupine** : keyword detection
- **Chromadb** : a vector search database that serves as the model memory
- **default LLM** : Qwen3 14B or other variants, GPTQ 4Bit quantized
- **Pytorch**
- **FastAPI**

## üí° Specifics

### Material :
  - **client side** :<br>
    for local mode :<br>
      - Jabra Speak2 (tested)
    for distributed mode :<br>
      - Raspberry Pi 5, 4GB
      - respeaker like from seeed studio
  - **server side** :
    - An Nvidia GPU with 16GB of VRAM (mini)

### Pipeline :
  - **t2yLLM** uses **AsyncLLMEngine from vLLM** in combination with **faster-whisper** in order to generate text from speech and stream tokens as fast as possible.
  - In distributed mode : The audio dispatcher processes text received from the LLM and transforms it to .flac segments and
    sends them to the client (raspberry Pi)
  - Sound reveived from the Jabra Speak2 or the Raspberry-Pi 5 is analyzed by **silerovad** to detect speech in addition to **pvporcupine**
  - Relevant sound is then translated by **Faster-Whisper** with low latency
  - The audio dispatcher transforms the LLM answer to speech with **piperTTS** and then sends audio parts in .flac
    over the network to reduce bandwidth usage and decrease latency

## ‚öôÔ∏è Parameters

- configuration should be done via the .yaml config file without having to directly interact with the code
- configuration can be enhanced via the YamlConfigLoader.py
- **t2yLLM** should be used on local network only since all is in clear text for now

## ‚öôÔ∏è Environment variables

create a .env file and use python-dotenv or edit your ~/.bashrc :

- export PORCUPINE_KEY='myporcupinekey'
- export OPENWEATHERMAP_API_KEY='myopenweatherkey'
- export VLLM_ATTENTION_BACKEND=FLASH_ATTN #for V1 engine
- export VLLM_FLASH_ATTN_VERSION="2"       #for V1 engine
- export VLLM_USE_V1=1
- VLLM_WORKER_MULTIPROC_METHOD="spawn" #for V1 engine
- export TORCH_CUDA_ARCH_LIST='myarchitecture' #if needed


## üîç Github links
<br>
Repositories used in t2yLLM project :<br>

- üîó [vLLM](https://github.com/vllm-project/vllm)
- üîó [Faster-Whisper](https://github.com/SYSTRAN/faster-whisper)
- üîó [Silero-vad](https://github.com/snakers4/silero-vad)
- üîó [pvporcupine](https://github.com/Picovoice/porcupine)
- üîó [Whisper-streaming](https://github.com/ufal/whisper_streaming)
- üîó [RealtimeSTT](https://github.com/KoljaB/RealtimeSTT)
- üîó [Chromadb](https://github.com/chroma-core/chroma)
- üîó [piperTTS](https://github.com/rhasspy/piper)
- üîó [json_repair](https://github.com/mangiucugna/json_repair)
- üîó [FastAPI](https://github.com/fastapi)
- üîó [pydantic](https://github.com/pydantic/pydantic)

## üîç APIs

- üîó [Tyradex](https://tyradex.vercel.app/)
- üîó [pokeapi](https://github.com/PokeAPI/pokeapi)
- üîó [OpenWeather](https://openweathermap.org/)

## Plugins

- Plugins can be added to the ./plugins folder. see example.py in ./plugins and pluginManager.py for implementation.
- Plugins have to be activated and deactivated via config
- List of **supported plugins** :
    - date
    - time
    - weather
    - wikipedia
    - pokemon
    - yeelight

## üõ†Ô∏è ToDo

- async memory handler for non blocking operations when dealing with memory
- WebOS and Xiaomi Yeelight plugins implementation
- Cut Voice when math formulas or code is detected
- use FastAPI errors
- use pydantic for UUID validation
- tkinter or equivalent interface for config

## ‚öñÔ∏è License

This code is under the **MIT** license. Please mention me as the author if you found this code useful
  [![MIT License](https://img.shields.io/badge/license-MIT-green)](https://opensource.org/licenses/MIT)

Copyright (c) 2025 Saga9103

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
